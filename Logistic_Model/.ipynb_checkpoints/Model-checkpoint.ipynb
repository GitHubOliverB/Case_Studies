{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic Python Libs - Not everything will be used here...\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import csv\n",
    "import pydotplus\n",
    "import plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Own Imports\n",
    "from Parameters import *\n",
    "from Classifier_Trainer import *\n",
    "from Classifier_Validation_Plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to train a model. Before I can do that I need to label and encode the categorial variables (Since categorial features are not supported in sklearn...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths, functions and other useful stuff\n",
    "data_path = \"..\\\\Data\\\\\"\n",
    "file_name = \"challenge_data.csv\"\n",
    "\n",
    "classifier_path = classifier_name[0]\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.makedirs(classifier_path)\n",
    "model_path = classifier_path+\"\\Model\\\\\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "plot_path = classifier_path+\"\\Plots\"\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "tree_path = plot_path+\"\\Decision_Trees\\\\\"\n",
    "if not os.path.exists(tree_path):\n",
    "    os.makedirs(tree_path)\n",
    "correlation_path = plot_path+\"\\Correlations\"\n",
    "if not os.path.exists(correlation_path):\n",
    "    os.makedirs(correlation_path)\n",
    "training_path = plot_path+\"\\Training\\\\\"\n",
    "if not os.path.exists(training_path):\n",
    "    os.makedirs(training_path)\n",
    "validation_path = plot_path+\"\\Validation\\\\\"\n",
    "if not os.path.exists(validation_path):\n",
    "    os.makedirs(validation_path)\n",
    "visualization_path = plot_path+\"\\Visualization\\\\\"\n",
    "if not os.path.exists(visualization_path):\n",
    "    os.makedirs(visualization_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'height_in_cm', 'date_shipped', 'size', 'itret', 'weight_in_kg',\n",
      "       'price', 'category_Belts', 'category_Blazers', 'category_Coats',\n",
      "       ...\n",
      "       'brand_Brand83', 'brand_Brand9', 'shipping_country_0',\n",
      "       'shipping_country_1', 'shipping_country_2', 'shipping_country_3',\n",
      "       'shipping_country_4', 'shipping_country_5', 'shipping_country_6',\n",
      "       'shipping_country_7'],\n",
      "      dtype='object', length=178)\n"
     ]
    }
   ],
   "source": [
    "# Data import and dummy variables\n",
    "df = pd.read_csv(data_path+file_name, delimiter=',')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "cat_cols = ['category', 'product_group', 'color', 'brand', 'shipping_country']\n",
    "df_dummies = pd.get_dummies(df, prefix_sep=\"_\",\n",
    "                              columns=cat_cols)\n",
    "\n",
    "size_values = ['S', 'M', 'L', 'XL', 'XXL']\n",
    "size_label = ['0', '1', '2', '3', '4']\n",
    "df_dummies = df_dummies.replace(to_replace=size_values, value=size_label)\n",
    "print(df_dummies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our dummy variables and we labled our size feature accordingly.\n",
    "In order to train our model, we will need to split our dataset into at least one training and one testing sample. I would invest more time in a better splitting, but given the timeframe I'll settle with a very simple splitting.\n",
    "Next up we define our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['brand_good', 'brand_bad', 'product_group_bad', 'color_bad'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d403c8c3dc01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(Feature_List)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dummies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFeature_List\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Indepent variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dummies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'itret'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Dependent Variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2934\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['brand_good', 'brand_bad', 'product_group_bad', 'color_bad'] not in index\""
     ]
    }
   ],
   "source": [
    "#Feature_List = df_dummies.columns.tolist()\n",
    "#Feature_List.remove('itret')\n",
    "#Feature_List.remove('date_shipped')\n",
    "#print(Feature_List)\n",
    "\n",
    "X = df_dummies[Feature_List] # Indepent variables\n",
    "y = df_dummies['itret'] # Dependent Variables\n",
    "\n",
    "# Split\n",
    "X_dev, X_eval, y_dev, y_eval = train_test_split(X, y, test_size=reserve_fraction, random_state=42)\n",
    "print(\"\\nReserving \" + str(format(reserve_fraction*100, '.2f')) + \"% Of Events For Later Use: \" + str(format(len(X_eval), ',.0f')))\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*100, '.2f')) + \"% Of Events For Splitting: \" + str(format(len(X_dev), ',.0f')))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=test_fraction, random_state=random_seeds[0])\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*(1-test_fraction)*100, '.2f')) + \"% Of Events For Training: \" + str(format(len(X_train), ',.0f')))\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*test_fraction*100, '.2f')) + \"% Of Events For Testing: \" + str(format(len(X_test), ',.0f')))\n",
    "print(\"\\nData Has Been Split...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"min_samples_leaf\": np.linspace(0.05, 0.2, 4),\n",
    "#     \"max_depth\": range(2,6),\n",
    "#     \"criterion\": [\"friedman_mse\"],\n",
    "#     \"subsample\":[0.8, 0.9, 1.0],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#     \"n_estimators\": np.arange(50, 300, 50)\n",
    "#     }\n",
    "\n",
    "\n",
    "# # run randomized search\n",
    "# print(\"Defining the Gridsearch\")\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='roc_auc', cv=3, n_jobs=4, verbose=2)\n",
    "# print(\"Training on all gridpoints...\")\n",
    "# clf.fit(X_train, y_train)\n",
    "# print('Gradient boosting trees best params:', clf.best_params_)\n",
    "# print('Gradient boosting trees score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model, train it and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GB_clfs = {}\n",
    "# GB_clfs[str(classifier_name[0])] = GradientBoostingClassifier(max_depth=max_depth, \n",
    "#                                                               min_samples_leaf=min_samples_leaf, \n",
    "#                                                               random_state=0, subsample=subsample, \n",
    "#                                                               n_estimators=n_estimators, \n",
    "#                                                               learning_rate=learning_rate, \n",
    "#                                                               verbose=True)\n",
    "# clfs=[]\n",
    "# print(\"\\nCrosstraining - 0\")\n",
    "# classifier_training(X_train, y_train, X_test, y_test, clfs, 0, \n",
    "#                     GB_clfs[classifier_name[0]], model_path, tree_path, training_path, False)\n",
    "# print(\"---\"*42)\n",
    "# print(\"Crosstraining - 1\")\n",
    "# classifier_training(X_train, y_train, X_test, y_test,clfs,  1, \n",
    "#                     GB_clfs[classifier_name[0]], model_path, tree_path, training_path, False)\n",
    "# print(\"\\nFinished Training...\")\n",
    "# print(\"---\"*42)\n",
    "# clfs[0].set_params(verbose=False)\n",
    "# clfs[1].set_params(verbose=False)\n",
    "\n",
    "logisticRegr = LogisticRegression(solver = 'liblinear', max_iter = 5000)\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions, target_names=[\"background\", \"signal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ntree_ROC_Curve_Figname = validation_path + str(classifier_name[0]) + \"_Tree_\" + str(len(clfs[0].estimators_)) + \"_ROC.png\"\n",
    "plot_Ntree_ROC_curve(clfs[0], (X_train,y_train),(X_test,y_test), Ntree_ROC_Curve_Figname)\n",
    "Ntree_PR_Curve_Figname = validation_path + str(classifier_name[0]) + \"_Tree_\" + str(len(clfs[0].estimators_)) + \"_PR.png\"\n",
    "plot_Ntree_PR_curve(clfs[0], (X_train,y_train),(X_test,y_test), Ntree_PR_Curve_Figname)\n",
    "\n",
    "# Reference For Some Of These Numbers\n",
    "# P - condition positive, the number of real positive cases in the data\n",
    "# N - condition negative, the number of real negative cases in the data\n",
    "\n",
    "# TP - true positive, eqv. with hit\n",
    "\n",
    "# TN - true negative,  eqv. with correct rejection\n",
    "# FP - false positive, eqv. with false alarm, Type I error\n",
    "# FN - false negative,  eqv. with miss, Type II error\n",
    "\n",
    "# PPV - Precision = TP / (TP + FP) = 1 - FDR, positive predictive value\n",
    "# TPR - Recall = TP / (TP + FN) = 1 - FNR, true positive rate\n",
    "# FNR - Miss Rate = FN / (FN + TP) = 1 - TPR, false negative rate\n",
    "# FDR - False Discovery Rate = FP / (FP + TP)\n",
    "# ACC - Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# F betta - F betta Score = (1+ betta**2) * ( PPV * TPR) / (betta**2 * PPV + TPR), is the harmonic mean of precision and recall\n",
    "\n",
    "# If I ever want to plot multiple clfs\n",
    "#fig, axes = plt.subplots(nrows=len(clfs), sharex=True)\n",
    "#for clf, ax in zip(clfs, axes):  \n",
    "#\tplot_learning_curve(clf, \"Learning curves\", X_dev, y_dev, scoring='roc_auc', n_jobs=7, cv=4, ax=ax, xlabel=False)\n",
    "#axes[0].legend(loc=\"best\")\n",
    "#axes[-1].set_xlabel(\"Training examples\")\n",
    "\n",
    "print(\"Plotting Performance vs. Size Of Training Set...\")\n",
    "fig, axis = plt.subplots(nrows=1, sharex=True)\n",
    "plot_learning_curve(clfs[0], \"Learning curves\", X_dev, y_dev, n_jobs=n_jobs, cv=cv, ax=axis)\n",
    "axis.legend(loc=\"best\")\n",
    "axis.set_xlabel(\"Training examples\")\n",
    "fig_name = validation_path + str(classifier_name[0]) + \"_Score_Validation\" + \".png\"\n",
    "plt.savefig(fig_name)\n",
    "plt.close()\n",
    "print(\"\\nFinished Validation...\")\n",
    "print(\"---\"*42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
