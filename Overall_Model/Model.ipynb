{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\oli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning:\n",
      "\n",
      "The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Python Libs - Not everything will be used here...\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import csv\n",
    "import pydotplus\n",
    "import plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, roc_auc_score, precision_recall_curve\n",
    "import joblib\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Own Imports\n",
    "from Parameters import *\n",
    "from Classifier_Trainer import *\n",
    "from Classifier_Validation_Plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to train a model. Before I can do that I need to label and encode the categorial variables (Since categorial features are not supported in sklearn...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths, functions and other useful stuff\n",
    "data_path = \"..\\\\Data\\\\\"\n",
    "file_name = \"challenge_data.csv\"\n",
    "\n",
    "classifier_path = classifier_name[0]\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.makedirs(classifier_path)\n",
    "model_path = classifier_path+\"\\Model\\\\\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "plot_path = classifier_path+\"\\Plots\"\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "tree_path = plot_path+\"\\Decision_Trees\\\\\"\n",
    "if not os.path.exists(tree_path):\n",
    "    os.makedirs(tree_path)\n",
    "correlation_path = plot_path+\"\\Correlations\"\n",
    "if not os.path.exists(correlation_path):\n",
    "    os.makedirs(correlation_path)\n",
    "training_path = plot_path+\"\\Training\\\\\"\n",
    "if not os.path.exists(training_path):\n",
    "    os.makedirs(training_path)\n",
    "validation_path = plot_path+\"\\Validation\\\\\"\n",
    "if not os.path.exists(validation_path):\n",
    "    os.makedirs(validation_path)\n",
    "visualization_path = plot_path+\"\\Visualization\\\\\"\n",
    "if not os.path.exists(visualization_path):\n",
    "    os.makedirs(visualization_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data import and dummy variables\n",
    "df = pd.read_csv(data_path+file_name, delimiter=',')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Regrouping\n",
    "df['brand']=np.where((df['brand'] == 'Brand78') |\n",
    "                     (df['brand'] == 'Brand75') |\n",
    "                     (df['brand'] == 'Brand56') |\n",
    "                     (df['brand'] == 'Brand71') |\n",
    "                     (df['brand'] == 'Brand70') |\n",
    "                     (df['brand'] == 'Brand65') |\n",
    "                     (df['brand'] == 'Brand82') |\n",
    "                     (df['brand'] == 'Brand69') |\n",
    "                     (df['brand'] == 'Brand51') |\n",
    "                     (df['brand'] == 'Brand64') |\n",
    "                     (df['brand'] == 'Brand37') |\n",
    "                     (df['brand'] == 'Brand44') |\n",
    "                     (df['brand'] == 'Brand19') |\n",
    "                     (df['brand'] == 'Brand35') |\n",
    "                     (df['brand'] == 'Brand33') |\n",
    "                     (df['brand'] == 'Brand54') |\n",
    "                     (df['brand'] == 'Brand25') |\n",
    "                     (df['brand'] == 'Brand13') |\n",
    "                     (df['brand'] == 'Brand45') |\n",
    "                     (df['brand'] == 'Brand60') |\n",
    "                     (df['brand'] == 'Brand59') |\n",
    "                     (df['brand'] == 'Brand72') |\n",
    "                     (df['brand'] == 'Brand26') |\n",
    "                     (df['brand'] == 'Brand50') |\n",
    "                     (df['brand'] == 'Brand63') |\n",
    "                     (df['brand'] == 'Brand41') |\n",
    "                     (df['brand'] == 'Brand68') |\n",
    "                     (df['brand'] == 'Brand46') |\n",
    "                     (df['brand'] == 'Brand9') |\n",
    "                     (df['brand'] == 'Brand5') |\n",
    "                     (df['brand'] == 'Brand67') |\n",
    "                     (df['brand'] == 'Brand38') |\n",
    "                     (df['brand'] == 'Brand49') |\n",
    "                     (df['brand'] == 'Brand30') |\n",
    "                     (df['brand'] == 'Brand12') |\n",
    "                     (df['brand'] == 'Brand31') |\n",
    "                     (df['brand'] == 'Brand40') |\n",
    "                     (df['brand'] == 'Brand52') |\n",
    "                     (df['brand'] == 'Brand47') |\n",
    "                     (df['brand'] == 'Brand39') |\n",
    "                     (df['brand'] == 'Brand10') |\n",
    "                     (df['brand'] == 'Brand79') |\n",
    "                     (df['brand'] == 'Brand83'),\n",
    "                     'bad',\n",
    "                     df['brand'])\n",
    "\n",
    "df['brand']=np.where((df['brand'] == 'Brand17') |\n",
    "                     (df['brand'] == 'Brand21') |\n",
    "                     (df['brand'] == 'Brand4') |\n",
    "                     (df['brand'] == 'Brand14') |\n",
    "                     (df['brand'] == 'Brand16') |\n",
    "                     (df['brand'] == 'Brand77') |\n",
    "                     (df['brand'] == 'Brand8') |\n",
    "                     (df['brand'] == 'Brand0') |\n",
    "                     (df['brand'] == 'Brand80') |\n",
    "                     (df['brand'] == 'Brand58') |\n",
    "                     (df['brand'] == 'Brand53') |\n",
    "                     (df['brand'] == 'Brand43') |\n",
    "                     (df['brand'] == 'Brand22') |\n",
    "                     (df['brand'] == 'Brand74') |\n",
    "                     (df['brand'] == 'Brand57') |\n",
    "                     (df['brand'] == 'Brand55') |\n",
    "                     (df['brand'] == 'Brand18') |\n",
    "                     (df['brand'] == 'Brand20') |\n",
    "                     (df['brand'] == 'Brand28'),\n",
    "                     'medium',\n",
    "                     df['brand'])\n",
    "\n",
    "df['brand']=np.where((df['brand'] == 'Brand1') |\n",
    "                     (df['brand'] == 'Brand11') |\n",
    "                     (df['brand'] == 'Brand76') |\n",
    "                     (df['brand'] == 'Brand29') |\n",
    "                     (df['brand'] == 'Brand42') |\n",
    "                     (df['brand'] == 'Brand7') |\n",
    "                     (df['brand'] == 'Brand48') |\n",
    "                     (df['brand'] == 'Brand6') |\n",
    "                     (df['brand'] == 'Brand27') |\n",
    "                     (df['brand'] == 'Brand32') |\n",
    "                     (df['brand'] == 'Brand15') |\n",
    "                     (df['brand'] == 'Brand3') |\n",
    "                     (df['brand'] == 'Brand23') |\n",
    "                     (df['brand'] == 'Brand81') |\n",
    "                     (df['brand'] == 'Brand62') |\n",
    "                     (df['brand'] == 'Brand61') |\n",
    "                     (df['brand'] == 'Brand66') |\n",
    "                     (df['brand'] == 'Brand24') |\n",
    "                     (df['brand'] == 'Brand34'),\n",
    "                     'good',\n",
    "                     df['brand'])\n",
    "\n",
    "# print(df['brand'].unique())\n",
    "# for item in df['brand'].unique():\n",
    "#     print(len(df['brand'][df.brand == item])/len(df.brand))\n",
    "\n",
    "\n",
    "df['color']=np.where((df['color'] == 'Metal') |\n",
    "                     (df['color'] == 'Black') |\n",
    "                     (df['color'] == 'White') |\n",
    "                     (df['color'] == 'Pink'),\n",
    "                     'bad',\n",
    "                     df['color'])\n",
    "\n",
    "df['color']=np.where((df['color'] == 'Grey') |\n",
    "                     (df['color'] == 'Turquoise') |\n",
    "                     (df['color'] == 'Beige') |\n",
    "                     (df['color'] == 'Multicolor') |\n",
    "                     (df['color'] == 'Brown') |\n",
    "                     (df['color'] == 'Yellow') |\n",
    "                     (df['color'] == 'Violet'),\n",
    "                     'medium',\n",
    "                     df['color'])\n",
    "\n",
    "df['color']=np.where((df['color'] == 'Green') |\n",
    "                     (df['color'] == 'Red') |\n",
    "                     (df['color'] == 'Orange'),\n",
    "                     'good',\n",
    "                     df['color'])\n",
    "\n",
    "# print(df['color'].unique())\n",
    "# for item in df['color'].unique():\n",
    "#     print(len(df['color'][df.color == item])/len(df.color))\n",
    "\n",
    "\n",
    "df['product_group']=np.where((df['product_group'] == 'Denim jacket') |\n",
    "                     (df['product_group'] == 'Caban') |\n",
    "                     (df['product_group'] == 'Trench coat') |\n",
    "                     (df['product_group'] == 'Short Coat') |\n",
    "                     (df['product_group'] == 'Blouson') |\n",
    "                     (df['product_group'] == 'Winter coat') |\n",
    "                     (df['product_group'] == 'Leather Jacket') |\n",
    "                     (df['product_group'] == 'Field jacket') |\n",
    "                     (df['product_group'] == 'Functional jacket') |\n",
    "                     (df['product_group'] == 'Blazer casual') |\n",
    "                     (df['product_group'] == 'Chino slim fit') |\n",
    "                     (df['product_group'] == 'Winter jacket') |\n",
    "                     (df['product_group'] == 'Light coat') |\n",
    "                     (df['product_group'] == 'Light jacket') |\n",
    "                     (df['product_group'] == 'Parka') |\n",
    "                     (df['product_group'] == 'PU-Jacket') |\n",
    "                     (df['product_group'] == 'Down jacket') |\n",
    "                     (df['product_group'] == 'Outdoor Vest') |\n",
    "                     (df['product_group'] == 'Belt casual') |\n",
    "                     (df['product_group'] == 'Cardigan') |\n",
    "                     (df['product_group'] == 'T-shirt long sleeves') |\n",
    "                     (df['product_group'] == 'Blazer Knit') |\n",
    "                     (df['product_group'] == 'Sweatpants') |\n",
    "                     (df['product_group'] == 'Cloth pants') |\n",
    "                     (df['product_group'] == 'Indoor Vest'),\n",
    "                     'bad',\n",
    "                     df['product_group'])\n",
    "\n",
    "df['product_group']=np.where((df['product_group'] == 'Medium fit') |\n",
    "                     (df['product_group'] == 'Briefs') |\n",
    "                     (df['product_group'] == 'Polo shirt longsleeves') |\n",
    "                     (df['product_group'] == 'Slim fit') |\n",
    "                     (df['product_group'] == 'Regular fit') |\n",
    "                     (df['product_group'] == 'T-shirt Print') |\n",
    "                     (df['product_group'] == 'Sweatshirt') |\n",
    "                     (df['product_group'] == 'Boxers') |\n",
    "                     (df['product_group'] == 'Troyer') |\n",
    "                     (df['product_group'] == 'Polo shirt shortsleeves') |\n",
    "                     (df['product_group'] == 'Hoodie') |\n",
    "                     (df['product_group'] == 'V-Neck') |\n",
    "                     (df['product_group'] == 'T-shirt Basic') |\n",
    "                     (df['product_group'] == 'C-Neck'),\n",
    "                     'medium',\n",
    "                     df['product_group'])\n",
    "\n",
    "df['product_group']=np.where((df['product_group'] == 'T-shirt striped / patterned') |\n",
    "                     (df['product_group'] == 'Chino shorts') |\n",
    "                     (df['product_group'] == 'Undershirt') |\n",
    "                     (df['product_group'] == 'Chino regular fit') |\n",
    "                     (df['product_group'] == 'Turtle neck') |\n",
    "                     (df['product_group'] == 'Swim shorts') |\n",
    "                     (df['product_group'] == 'Sweat jacket') |\n",
    "                     (df['product_group'] == 'Trunks') |\n",
    "                     (df['product_group'] == 'Casual shirt short sleeves') |\n",
    "                     (df['product_group'] == 'Jeans shorts') |\n",
    "                     (df['product_group'] == 'Business shirt short sleeves') |\n",
    "                     (df['product_group'] == 'Cargo Shorts'),\n",
    "                     'good',\n",
    "                     df['product_group'])\n",
    "\n",
    "# print(df['product_group'].unique())\n",
    "# for item in df['product_group'].unique():\n",
    "#     print(len(df['product_group'][df.product_group == item])/len(df.product_group))\n",
    "\n",
    "    \n",
    "df['category']=np.where((df['category'] == 'Coats') |\n",
    "                     (df['category'] == 'Leather Jackets') |\n",
    "                     (df['category'] == 'Blazers') |\n",
    "                     (df['category'] == 'Jackets') |\n",
    "                     (df['category'] == 'Vest') |\n",
    "                     (df['category'] == 'Belts'),\n",
    "                     'bad',\n",
    "                     df['category'])\n",
    "\n",
    "df['category']=np.where((df['category'] == 'Pants') |\n",
    "                     (df['category'] == 'T-Shirts') |\n",
    "                     (df['category'] == 'Knitted tops') |\n",
    "                     (df['category'] == 'Shirts business'),\n",
    "                     'medium',\n",
    "                     df['category'])\n",
    "\n",
    "df['category']=np.where((df['category'] == 'Sweaters') |\n",
    "                     (df['category'] == 'Underwear') |\n",
    "                     (df['category'] == 'Shorts') |\n",
    "                     (df['category'] == 'Shirt short sleeves'),\n",
    "                     'good',\n",
    "                     df['category'])\n",
    "\n",
    "# print(df['category'].unique())\n",
    "# for item in df['category'].unique():\n",
    "#     print(len(df['category'][df.category == item])/len(df.category))    \n",
    "\n",
    "cat_cols = ['category', 'product_group', 'color', 'brand', 'shipping_country']\n",
    "df_dummies = pd.get_dummies(df, prefix_sep=\"_\",\n",
    "                              columns=cat_cols)\n",
    "\n",
    "size_values = ['S', 'M', 'L', 'XL', 'XXL']\n",
    "size_label = ['0', '1', '2', '3', '4']\n",
    "df_dummies = df_dummies.replace(to_replace=size_values, value=size_label)\n",
    "#print(df_dummies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our dummy variables and we labled our size feature accordingly.\n",
    "In order to train our model, we will need to split our dataset into at least one training and one testing sample. I would invest more time in a better splitting, but given the timeframe I'll settle with a very simple splitting.\n",
    "Next up we define our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reserving 5.00% Of Events For Later Use: 7,344\n",
      "\n",
      "Using 95.00% Of Events For Splitting: 139,522\n",
      "\n",
      "Using 47.50% Of Events For Training: 69,761\n",
      "\n",
      "Using 47.50% Of Events For Testing: 69,761\n",
      "\n",
      "Data Has Been Split...\n"
     ]
    }
   ],
   "source": [
    "#Feature_List = df_dummies.columns.tolist()\n",
    "#Feature_List.remove('itret')\n",
    "#Feature_List.remove('date_shipped')\n",
    "#print(Feature_List)\n",
    "\n",
    "X = df_dummies[Feature_List] # Indepent variables\n",
    "y = df_dummies['itret'] # Dependent Variables\n",
    "\n",
    "# Split\n",
    "X_dev, X_eval, y_dev, y_eval = train_test_split(X, y, test_size=reserve_fraction, random_state=42)\n",
    "print(\"\\nReserving \" + str(format(reserve_fraction*100, '.2f')) + \"% Of Events For Later Use: \" + str(format(len(X_eval), ',.0f')))\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*100, '.2f')) + \"% Of Events For Splitting: \" + str(format(len(X_dev), ',.0f')))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=test_fraction, random_state=random_seeds[0])\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*(1-test_fraction)*100, '.2f')) + \"% Of Events For Training: \" + str(format(len(X_train), ',.0f')))\n",
    "print(\"\\nUsing \" + str(format((1-reserve_fraction)*test_fraction*100, '.2f')) + \"% Of Events For Testing: \" + str(format(len(X_test), ',.0f')))\n",
    "print(\"\\nData Has Been Split...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"min_samples_leaf\": np.linspace(0.05, 0.2, 4),\n",
    "#     \"max_depth\": range(2,6),\n",
    "#     \"criterion\": [\"friedman_mse\"],\n",
    "#     \"subsample\":[0.8, 0.9, 1.0],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#     \"n_estimators\": np.arange(50, 300, 50)\n",
    "#     }\n",
    "\n",
    "\n",
    "# # run randomized search\n",
    "# print(\"Defining the Gridsearch\")\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='roc_auc', cv=3, n_jobs=4, verbose=2)\n",
    "# print(\"Training on all gridpoints...\")\n",
    "# clf.fit(X_train, y_train)\n",
    "# print('Gradient boosting trees best params:', clf.best_params_)\n",
    "# print('Gradient boosting trees score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model, train it and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crosstraining - 0\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3665           13.29s\n",
      "         2           1.3501           12.67s\n",
      "         3           1.3362           12.93s\n",
      "         4           1.3236           13.46s\n",
      "         5           1.3132           13.15s\n",
      "         6           1.3035           12.95s\n",
      "         7           1.2957           12.68s\n",
      "         8           1.2881           12.64s\n",
      "         9           1.2818           12.46s\n",
      "        10           1.2763           12.16s\n",
      "        20           1.2408           11.53s\n",
      "        30           1.2261           11.00s\n",
      "        40           1.2182           10.39s\n",
      "        50           1.2131            9.84s\n",
      "        60           1.2101            9.34s\n",
      "        70           1.2079            8.88s\n",
      "        80           1.2062            8.37s\n",
      "        90           1.2051            7.80s\n",
      "       100           1.2039            7.31s\n",
      "       200           1.1961            2.39s\n",
      "Safing trained model...\n",
      "Decision Trees can be saved as a PNG if wanted. Change Input to True.\n",
      "\n",
      "Feature Ranking By Variable Importance\n",
      "\n",
      "Rank\tVariable\t\tVariable Importance (+/- STD)\n",
      "---------------------------------------------------------------\n",
      "   1\tage\t\t\t0.2305 +/- 0.1362\n",
      "   2\tprice\t\t\t0.1506 +/- 0.1901\n",
      "   3\tproduct_group_bad\t\t\t0.1473 +/- 0.0843\n",
      "   4\tshipping_country_1\t\t\t0.1036 +/- 0.0717\n",
      "   5\tweight_in_kg\t\t\t0.0945 +/- 0.1386\n",
      "   6\tbrand_good\t\t\t0.0722 +/- 0.074\n",
      "   7\tbrand_bad\t\t\t0.0635 +/- 0.0668\n",
      "   8\tcategory_Shirts casual\t\t\t0.0617 +/- 0.0506\n",
      "   9\tcolor_bad\t\t\t0.0521 +/- 0.0608\n",
      "   10\theight_in_cm\t\t\t0.0241 +/- 0.148\n",
      "\n",
      "Classification For Crosstraining - 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.69      0.70      0.70     34863\n",
      "      signal       0.70      0.68      0.69     34898\n",
      "\n",
      "    accuracy                           0.69     69761\n",
      "   macro avg       0.69      0.69      0.69     69761\n",
      "weighted avg       0.69      0.69      0.69     69761\n",
      "\n",
      "Confusion Matrix on Testing Set:\n",
      "[[24502 10361]\n",
      " [11035 23863]]\n",
      "\n",
      "Area under ROC curve: 0.7431\n",
      "Area under PR curve:  0.7186\n",
      "\n",
      "Over/Underfitting Test:\n",
      "\n",
      "Testing signal efficiency compared to (training signal efficiency)\n",
      "\n",
      "@B=0.01\t\t\t@B=0.10\t\t\t@B=0.30\n",
      "-------------------------------------------------------------------------\n",
      "0.039 (0.046)\t\t0.355 (0.363)\t\t0.687 (0.692)\n",
      "\n",
      "Comparing BDT Output Distribution for Training and Testing Set\n",
      "\n",
      "\t\tSignal\t\t\tBackground\n",
      "-------------------------------------------------------------------------\n",
      "KS p-value \t0.83\t\t\t0.167\n",
      "\n",
      "Based on the p-value of the Kolmogorov-Smirnov Test...\n",
      "there seems to be no Over/Undertraining present for the Signal-Distribustions.\n",
      "\n",
      "there seems to be no Over/Undertraining present for the Background-Distribustions.\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Crosstraining - 1\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3669           13.41s\n",
      "         2           1.3502           12.84s\n",
      "         3           1.3370           13.20s\n",
      "         4           1.3244           13.03s\n",
      "         5           1.3139           12.83s\n",
      "         6           1.3044           12.94s\n",
      "         7           1.2962           12.60s\n",
      "         8           1.2890           12.52s\n",
      "         9           1.2825           12.34s\n",
      "        10           1.2768           12.33s\n",
      "        20           1.2414           11.41s\n",
      "        30           1.2260           10.89s\n",
      "        40           1.2176           10.31s\n",
      "        50           1.2128            9.77s\n",
      "        60           1.2092            9.29s\n",
      "        70           1.2068            8.76s\n",
      "        80           1.2050            8.27s\n",
      "        90           1.2037            7.76s\n",
      "       100           1.2024            7.28s\n",
      "       200           1.1945            2.39s\n",
      "Safing trained model...\n",
      "Decision Trees can be saved as a PNG if wanted. Change Input to True.\n",
      "\n",
      "Feature Ranking By Variable Importance\n",
      "\n",
      "Rank\tVariable\t\tVariable Importance (+/- STD)\n",
      "---------------------------------------------------------------\n",
      "   1\tage\t\t\t0.2318 +/- 0.1447\n",
      "   2\tprice\t\t\t0.1703 +/- 0.1684\n",
      "   3\tproduct_group_bad\t\t\t0.1383 +/- 0.0936\n",
      "   4\tweight_in_kg\t\t\t0.0998 +/- 0.1339\n",
      "   5\tshipping_country_1\t\t\t0.0906 +/- 0.0648\n",
      "   6\tbrand_good\t\t\t0.0698 +/- 0.087\n",
      "   7\tcategory_Shirts casual\t\t\t0.0664 +/- 0.0538\n",
      "   8\tbrand_bad\t\t\t0.0553 +/- 0.0702\n",
      "   9\tcolor_bad\t\t\t0.0512 +/- 0.0553\n",
      "   10\theight_in_cm\t\t\t0.0265 +/- 0.158\n",
      "\n",
      "Classification For Crosstraining - 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.69      0.70      0.69     34974\n",
      "      signal       0.69      0.68      0.69     34787\n",
      "\n",
      "    accuracy                           0.69     69761\n",
      "   macro avg       0.69      0.69      0.69     69761\n",
      "weighted avg       0.69      0.69      0.69     69761\n",
      "\n",
      "Confusion Matrix on Training Set:\n",
      "[[24508 10466]\n",
      " [11150 23637]]\n",
      "\n",
      "Area under ROC curve: 0.7416\n",
      "Area under PR curve:  0.7172\n",
      "\n",
      "Over/Underfitting Test:\n",
      "\n",
      "Testing signal efficiency compared to (training signal efficiency)\n",
      "\n",
      "@B=0.01\t\t\t@B=0.10\t\t\t@B=0.30\n",
      "-------------------------------------------------------------------------\n",
      "0.037 (0.042)\t\t0.355 (0.364)\t\t0.68 (0.697)\n",
      "WARNING: The difference between training and testing efficiency @B=.3 is a bit high.\n",
      "The model may be a bit over/undertrained!\n",
      "\n",
      "Comparing BDT Output Distribution for Training and Testing Set\n",
      "\n",
      "\t\tSignal\t\t\tBackground\n",
      "-------------------------------------------------------------------------\n",
      "KS p-value \t0.004\t\t\t0.478\n",
      "\n",
      "Based on the p-value of the Kolmogorov-Smirnov Test...\n",
      "WARNING: Over/Undertraining present for the Signal-Distribustions!\n",
      "\n",
      "there seems to be no Over/Undertraining present for the Background-Distribustions.\n",
      "\n",
      "Finished Training...\n",
      "------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=0.05, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=False,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_clfs = {}\n",
    "GB_clfs[str(classifier_name[0])] = GradientBoostingClassifier(max_depth=max_depth, \n",
    "                                                              min_samples_leaf=min_samples_leaf, \n",
    "                                                              random_state=0, subsample=subsample, \n",
    "                                                              n_estimators=n_estimators, \n",
    "                                                              learning_rate=learning_rate, \n",
    "                                                              verbose=True)\n",
    "clfs=[]\n",
    "print(\"\\nCrosstraining - 0\")\n",
    "classifier_training(X_train, y_train, X_test, y_test, clfs, 0, \n",
    "                    GB_clfs[classifier_name[0]], model_path, tree_path, training_path, False)\n",
    "print(\"---\"*42)\n",
    "print(\"Crosstraining - 1\")\n",
    "classifier_training(X_train, y_train, X_test, y_test,clfs,  1, \n",
    "                    GB_clfs[classifier_name[0]], model_path, tree_path, training_path, False)\n",
    "print(\"\\nFinished Training...\")\n",
    "print(\"---\"*42)\n",
    "clfs[0].set_params(verbose=False)\n",
    "clfs[1].set_params(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest ROC-AUC At Tree: 250\n",
      "Highest PR-AUC At Tree: 250\n",
      "Plotting Performance vs. Size Of Training Set...\n",
      "[learning_curve] Training set sizes: [ 9301 18602 27904 37205 46507 55808 65109 74411 83712 93014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Validation...\n",
      "------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "Ntree_ROC_Curve_Figname = validation_path + str(classifier_name[0]) + \"_Tree_\" + str(len(clfs[0].estimators_)) + \"_ROC.png\"\n",
    "plot_Ntree_ROC_curve(clfs[0], (X_train,y_train),(X_test,y_test), Ntree_ROC_Curve_Figname)\n",
    "Ntree_PR_Curve_Figname = validation_path + str(classifier_name[0]) + \"_Tree_\" + str(len(clfs[0].estimators_)) + \"_PR.png\"\n",
    "plot_Ntree_PR_curve(clfs[0], (X_train,y_train),(X_test,y_test), Ntree_PR_Curve_Figname)\n",
    "\n",
    "# Reference For Some Of These Numbers\n",
    "# P - condition positive, the number of real positive cases in the data\n",
    "# N - condition negative, the number of real negative cases in the data\n",
    "\n",
    "# TP - true positive, eqv. with hit\n",
    "\n",
    "# TN - true negative,  eqv. with correct rejection\n",
    "# FP - false positive, eqv. with false alarm, Type I error\n",
    "# FN - false negative,  eqv. with miss, Type II error\n",
    "\n",
    "# PPV - Precision = TP / (TP + FP) = 1 - FDR, positive predictive value\n",
    "# TPR - Recall = TP / (TP + FN) = 1 - FNR, true positive rate\n",
    "# FNR - Miss Rate = FN / (FN + TP) = 1 - TPR, false negative rate\n",
    "# FDR - False Discovery Rate = FP / (FP + TP)\n",
    "# ACC - Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# F betta - F betta Score = (1+ betta**2) * ( PPV * TPR) / (betta**2 * PPV + TPR), is the harmonic mean of precision and recall\n",
    "\n",
    "# If I ever want to plot multiple clfs\n",
    "#fig, axes = plt.subplots(nrows=len(clfs), sharex=True)\n",
    "#for clf, ax in zip(clfs, axes):  \n",
    "#\tplot_learning_curve(clf, \"Learning curves\", X_dev, y_dev, scoring='roc_auc', n_jobs=7, cv=4, ax=ax, xlabel=False)\n",
    "#axes[0].legend(loc=\"best\")\n",
    "#axes[-1].set_xlabel(\"Training examples\")\n",
    "\n",
    "print(\"Plotting Performance vs. Size Of Training Set...\")\n",
    "fig, axis = plt.subplots(nrows=1, sharex=True)\n",
    "plot_learning_curve(clfs[0], \"Learning curves\", X_dev, y_dev, n_jobs=n_jobs, cv=cv, ax=axis)\n",
    "axis.legend(loc=\"best\")\n",
    "axis.set_xlabel(\"Training examples\")\n",
    "fig_name = validation_path + str(classifier_name[0]) + \"_Score_Validation\" + \".png\"\n",
    "plt.savefig(fig_name)\n",
    "plt.close()\n",
    "print(\"\\nFinished Validation...\")\n",
    "print(\"---\"*42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
